import logging
import streamlit as st
from langchain_community.document_loaders import YoutubeLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from youtube_transcript_api import YouTubeTranscriptApi
from langchain_community.llms import Ollama
from langchain.chains import LLMChain
import ollama  # Import Ollama's Python library
from langchain.embeddings.base import Embeddings

# Set up logging
logging.basicConfig(level=logging.INFO)

# Custom Embedding Class
class OllamaEmbeddings(Embeddings):
    def __init__(self, model_name: str = "mxbai-embed-large"):
        self.model_name = model_name

    def embed_documents(self, texts):
        embeddings = []
        for text in texts:
            response = ollama.embeddings(model=self.model_name, prompt=text)
            embedding = response["embedding"]
            embeddings.append(embedding)
        return embeddings

    def embed_query(self, text):
        response = ollama.embeddings(model=self.model_name, prompt=text)
        embedding = response["embedding"]
        return embedding

def initialize_model():
    """
    Initializes the Ollama LLM.
    Returns:
        Ollama: The language model.
    """
    llm = Ollama(model="llama3.2:1b", temperature=0.1)  # Replace with your actual model name (llama3.2:1b, gemma2:2b)
    return llm


def create_vector_from_youtube_url(video_url: str, language: str) -> FAISS:
    """
    Creates a vector representation of the given YouTube video URL.
    """
    try:
        logging.debug(f"Attempting to load transcript for video URL: {video_url} with language: {language}")
        loader = YoutubeLoader.from_youtube_url(video_url, language=language)
        transcript = loader.load()
        logging.debug(f"Transcript loaded: {transcript}")

        if not transcript:
            raise ValueError("Transcript is empty. The video may not have subtitles in the selected language.")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
        docs = text_splitter.split_documents(transcript)
        logging.debug(f"Number of documents after splitting: {len(docs)}")

        if not docs:
            raise ValueError("No documents were generated from the transcript.")

        # Initialize Ollama Embeddings
        embeddings = OllamaEmbeddings(model_name="mxbai-embed-large")

        # Create the vector store using Ollama embeddings
        db = FAISS.from_documents(docs, embeddings)
        return db
    except IndexError as e:
        st.error(f"IndexError occurred: {str(e)}")
        raise ValueError("Error creating vector from YouTube URL (List index out of range)")
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        raise ValueError(f"Error creating vector from YouTube URL: {str(e)}")


def get_response_from_query(db, query, language, k=4):
    """
    Retrieves a response to a query based on a similarity search in a database.
    Parameters:
    - db (Database): The database object used for similarity search.
    - query (str): The query string.
    - k (int): The number of documents to retrieve (default is 4).
    Returns:
    - response (str): The response generated by the YouTube assistant.
    - docs (list): The list of documents retrieved from the database.
    Raises:
    - None
    Example usage:
    response, docs = get_response_from_query(db, "How to cook pasta?")
    """
    try: 
        docs = db.similarity_search(query, k=k)
        docs_page_content = " ".join([doc.page_content for doc in docs])
        
        llm = initialize_model()
        
        language_str = 'Spanish' if language == 'es' else 'German' if language == 'de' else 'English'
        
        prompt_template = PromptTemplate(
            input_variables=["question", "docs", "language"],
            template="""
                    You are a helpful assistant that answers user questions based solely on the provided video transcript.

                    **Instructions:**

                    - Provide a clear and concise answer to the user's question.
                    - Use only the information relevant to the question.
                    - Do not include information that is not directly related to the question.
                    - Do not provide personal opinions or external information.
                    - Do not repeat or rephrase the question in your answer.
                    - Answer in {language} using markdown format.
                    - Begin your answer immediately without any preamble.
                    - Do not include the question in your answer.
                    - Do not explicitly mention the video or the transcript in your answer.

                    **Transcript:**

                    {docs}

                    **Question:**

                    {question}

                    **Answer:**
            """)
        
        # runnable sequence
        runnable_sequence = prompt_template | llm 
        
        # Run the LLMchain
        response = runnable_sequence.invoke({'question': query, 'docs': docs_page_content, 'language': language_str})
        response = response.replace("\n", " ")
        return response, docs
    except Exception as e:
        logging.error(f"An error occurred while generating the response: {str(e)}")
        st.error(f"An error occurred while generating the response: {str(e)}")
        raise ValueError(f"Error generating response: {str(e)}")